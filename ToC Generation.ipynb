{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Table of Contents Generation\n",
    " This notebook was created with the purpose of automatically generating table of contents from text.\n",
    " The toy dataset used to play with this problem was pulled from:\n",
    " http://jmcauley.ucsd.edu/data/amazon/\n",
    " \n",
    " This dataset contains reviews of books, and titles of reviews. The input to the model will be the block of review text and the output will be the short title of the review. This should give us a decent idea of the possibilities with table content generation. The current plan for the architecture is to name any section of text based on the location of certain words in the text. So the actual input will be one hot encoded text tokens and the output will be locations of words that should be used for the ToC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from collections import Counter\n",
    "from time import time\n",
    "    \n",
    "vocab_size = 1000\n",
    "max_sequence_len = 200\n",
    "max_title_len = 5\n",
    "\n",
    "data_string = open(\"../Downloads/reviews_Musical_Instruments_5.json\").read()\n",
    "raw_data = data_string.split(\"\\n\")\n",
    "data = [json.loads(d) for d in raw_data if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_long_string(obj):\n",
    "    s = \"\"\n",
    "    for example in obj:\n",
    "        s += example['summary']+\" \"\n",
    "        s += example['reviewText']+\" \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_index(text, num_words):\n",
    "    sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    word_counts = Counter(sequence)\n",
    "    top_words = word_counts.most_common(num_words)\n",
    "    word_index = dict((i+1,str(top_words[i][0])) for i in range(len(top_words)))\n",
    "    word_index[0] = \"UNK\"\n",
    "    return word_index, dict((v,k) for k,v in word_index.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot_tensor(sequence,word_number_index, max_length):\n",
    "    number_sequence = []\n",
    "    for word in sequence:\n",
    "        if word in word_number_index:\n",
    "            number_sequence.append(word_number_index[word])\n",
    "        else:\n",
    "            number_sequence.append(0) #append 0 for the UNK word\n",
    "    a = np.array(number_sequence)\n",
    "    b = np.zeros((1,max_length, len(word_number_index)))\n",
    "    b[0,np.arange(len(number_sequence)), a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_tensor_to_words(tensor, number_word_index):\n",
    "    sentence = \"\"\n",
    "    for i in range(len(tensor)):\n",
    "        if np.sum(tensor[i]):\n",
    "            sentence += number_word_index[np.argwhere(tensor[i])[0][0]]+\" \"\n",
    "    return sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_text(text, title, max_sequence_len, max_title_len):\n",
    "    #Make sure that the text is in the title for\n",
    "    #this base case and then make sure that the\n",
    "    #review is short enough\n",
    "    sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    title_seq = text_to_word_sequence(title, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    lt = len(title_seq)\n",
    "    ls = len(sequence)\n",
    "    if ls > max_sequence_len or lt > max_title_len or ls < 1 or lt < 1 :\n",
    "        return []\n",
    "    title_in_text = True\n",
    "    for word in title_seq:\n",
    "        if word not in sequence:\n",
    "            title_in_text = False\n",
    "    if title_in_text:\n",
    "        return [sequence, title_seq]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output_vector(text_seq, title_seq, max_sequence_len):\n",
    "    output = np.zeros((1,max_sequence_len))\n",
    "    num_words = len(title_seq)\n",
    "    order_change_value = 0.1/num_words\n",
    "    for i in range(num_words):\n",
    "        output[0,text_seq.index(title_seq[i])] = 1.0 - i*order_change_value\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title(output_vector, input_sequence):\n",
    "    copy = np.array(output_vector)\n",
    "    title = \"\"\n",
    "    while np.amax(copy) > 0.5:\n",
    "        i = np.argmax(copy)\n",
    "        title += input_sequence[i] + \" \"\n",
    "        copy[i] = 0\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_data(data, word_number_index,max_sequence_len,max_title_len):\n",
    "#     X = np.zeros((1,max_sequence_len,len(word_number_index)))\n",
    "#     y = np.zeros((1,max_sequence_len))\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        sequence = filter_text(data[i]['reviewText'], data[i]['summary'], max_sequence_len, max_title_len)\n",
    "        if sequence:\n",
    "            one_hot_input = create_one_hot_tensor(sequence[0],word_number_index, max_sequence_len)\n",
    "            output_vector = create_output_vector(sequence[0], sequence[1], max_sequence_len)\n",
    "            X.append(one_hot_input)\n",
    "            y.append(output_vector)\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = create_long_string(data[:1000])\n",
    "number_word_index, word_number_index = train_word_index(text,vocab_size-1)\n",
    "start = time()\n",
    "X, y = shape_data(data, word_number_index,max_sequence_len,max_title_len)\n",
    "print \"This took\", time()-start, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 12\n",
    "\n",
    "# the data, split between train and test sets\n",
    "x_train, y_train, x_test, y_test = X[:int(len(X)*0.8)],y[:int(len(y)*0.8)],X[int(len(X)*0.8):],y[int(len(y)*0.8):]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], max_sequence_len, vocab_size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], max_sequence_len, vocab_size, 1)\n",
    "input_shape = (max_sequence_len, vocab_size, 1)\n",
    "\n",
    "print(input_shape)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(max_sequence_len, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
