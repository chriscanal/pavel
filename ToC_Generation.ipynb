{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Table of Contents Generation\n",
    " This notebook was created with the purpose of automatically generating table of contents from text.\n",
    " The toy dataset used to play with this problem was pulled from:\n",
    " http://jmcauley.ucsd.edu/data/amazon/\n",
    " \n",
    " This dataset contains reviews of books, and titles of reviews. The input to the model will be the block of review text and the output will be the short title of the review. This should give us a decent idea of the possibilities with table content generation. The current plan for the architecture is to name any section of text based on the location of certain words in the text. So the actual input will be one hot encoded text tokens and the output will be locations of words that should be used for the ToC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from collections import Counter\n",
    "from time import time\n",
    "    \n",
    "vocab_size = 200\n",
    "max_sequence_len = 200\n",
    "max_title_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_string = open(\"../Downloads/reviews_Musical_Instruments_5.json\").read()\n",
    "raw_data = data_string.split(\"\\n\")\n",
    "data = [json.loads(d) for d in raw_data if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_long_string(obj):\n",
    "    s = \"\"\n",
    "    for example in obj:\n",
    "        s += example['summary']+\" \"\n",
    "        s += example['reviewText']+\" \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_index(text, num_words):\n",
    "    sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    word_counts = Counter(sequence)\n",
    "    top_words = word_counts.most_common(num_words)\n",
    "    word_index = dict((i+1,str(top_words[i][0])) for i in range(len(top_words)))\n",
    "    word_index[0] = \"UNK\"\n",
    "    return word_index, dict((v,k) for k,v in word_index.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot_tensor(sequence,word_number_index, max_length):\n",
    "    number_sequence = []\n",
    "    for word in sequence:\n",
    "        if word in word_number_index:\n",
    "            number_sequence.append(word_number_index[word])\n",
    "        else:\n",
    "            number_sequence.append(0) #append 0 for the UNK word\n",
    "    a = np.array(number_sequence)\n",
    "    b = np.zeros((1,max_length, len(word_number_index)))\n",
    "    b[0,np.arange(len(number_sequence)), a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_tensor_to_words(tensor, number_word_index):\n",
    "    sentence = \"\"\n",
    "    for i in range(len(tensor)):\n",
    "        if np.sum(tensor[i]):\n",
    "            sentence += number_word_index[np.argwhere(tensor[i])[0][0]]+\" \"\n",
    "    return sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_text(text, title, max_sequence_len, max_title_len):\n",
    "    #Make sure that the text is in the title for\n",
    "    #this base case and then make sure that the\n",
    "    #review is short enough\n",
    "    sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    title_seq = text_to_word_sequence(title, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    lt = len(title_seq)\n",
    "    ls = len(sequence)\n",
    "    if ls > max_sequence_len or lt > max_title_len or ls < 1 or lt < 1 :\n",
    "        return []\n",
    "    title_in_text = True\n",
    "    for word in title_seq:\n",
    "        if word not in sequence:\n",
    "            title_in_text = False\n",
    "    if title_in_text:\n",
    "        return [sequence, title_seq]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output_vector(text_seq, title_seq, max_sequence_len):\n",
    "    output = np.zeros((1,max_sequence_len))\n",
    "    num_words = len(title_seq)\n",
    "    order_change_value = 0.1/num_words\n",
    "    for i in range(num_words):\n",
    "        output[0,text_seq.index(title_seq[i])] = 1.0 - i*order_change_value\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title(output_vector, input_sequence):\n",
    "    copy = np.array(output_vector[0])\n",
    "    title = \"\"\n",
    "    while np.amax(copy) > 0.1:\n",
    "        i = np.argmax(copy)\n",
    "        title += input_sequence[i] + \" \"\n",
    "        copy[i] = 0\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_data(data, word_number_index,max_sequence_len,max_title_len):\n",
    "#     X = np.zeros((1,max_sequence_len,len(word_number_index)))\n",
    "#     y = np.zeros((1,max_sequence_len))\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        sequence = filter_text(data[i]['reviewText'], data[i]['summary'], max_sequence_len, max_title_len)\n",
    "        if sequence:\n",
    "            one_hot_input = create_one_hot_tensor(sequence[0],word_number_index, max_sequence_len)\n",
    "            output_vector = create_output_vector(sequence[0], sequence[1], max_sequence_len)\n",
    "            X.append(one_hot_input)\n",
    "            y.append(output_vector)\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 1.43782091141 seconds\n"
     ]
    }
   ],
   "source": [
    "text = create_long_string(data[:1000])\n",
    "number_word_index, word_number_index = train_word_index(text,vocab_size-1)\n",
    "start = time()\n",
    "X, y = shape_data(data, word_number_index,max_sequence_len,max_title_len)\n",
    "print \"This took\", time()-start, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 1)\n",
      "('x_train shape:', (1083, 200, 200, 1))\n",
      "(1083, 'train samples')\n",
      "(271, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 3\n",
    "\n",
    "# the data, split between train and test sets\n",
    "x_train, y_train, x_test, y_test = X[:int(len(X)*0.8)],y[:int(len(y)*0.8)],X[int(len(X)*0.8):],y[int(len(y)*0.8):]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], max_sequence_len, vocab_size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], max_sequence_len, vocab_size, 1)\n",
    "input_shape = (max_sequence_len, vocab_size, 1)\n",
    "\n",
    "print(input_shape)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(max_sequence_len, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1083 samples, validate on 271 samples\n",
      "Epoch 1/3\n",
      "1083/1083 [==============================] - 812s 750ms/step - loss: 0.0890 - acc: 0.9733 - val_loss: 0.0516 - val_acc: 0.9877\n",
      "Epoch 2/3\n",
      "1083/1083 [==============================] - 846s 782ms/step - loss: 0.0547 - acc: 0.9873 - val_loss: 0.0527 - val_acc: 0.9877\n",
      "Epoch 3/3\n",
      "1083/1083 [==============================] - 849s 784ms/step - loss: 0.0525 - acc: 0.9875 - val_loss: 0.0514 - val_acc: 0.9877\n",
      "('Test loss:', 0.051420163321978934)\n",
      "('Test accuracy:', 0.98767528190823939)\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text_title(text,model,max_sequence_len,word_number_index,vocab_size):\n",
    "    sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")\n",
    "    if len(sequence) > max_sequence_len:\n",
    "        return \"ERROR: The input text is too long for this model\"\n",
    "    else:\n",
    "        X = create_one_hot_tensor(sequence,word_number_index, max_sequence_len)\n",
    "        X = X.reshape(X.shape[0], max_sequence_len, vocab_size, 1)\n",
    "        y = model.predict(X)\n",
    "        print y\n",
    "        title = create_title(y, sequence)\n",
    "        print \"Input Text:\"\n",
    "        print text, \"\\n\\n\"\n",
    "        print \"The model produced the title:\", title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14211692  0.17014553  0.12502606  0.14736106  0.1954691   0.0853577\n",
      "   0.05704629  0.0351056   0.0380179   0.0285765   0.02389583  0.02320454\n",
      "   0.01747099  0.01997231  0.01662601  0.01796689  0.01021424  0.01059234\n",
      "   0.00967774  0.00745477  0.01204208  0.00477566  0.00465875  0.00706493\n",
      "   0.00467281  0.00472105  0.00409616  0.00523411  0.003194    0.00346611\n",
      "   0.0040099   0.00335662  0.00268254  0.00556993  0.00208265  0.00272021\n",
      "   0.0023888   0.00202976  0.0020385   0.00224964  0.00167014  0.00223631\n",
      "   0.00383977  0.00130097  0.00303147  0.00317695  0.00224009  0.00164138\n",
      "   0.00134431  0.00158017  0.00152349  0.00128965  0.0016482   0.00102297\n",
      "   0.00112634  0.0009228   0.00172286  0.00198497  0.001576    0.00119431\n",
      "   0.00093759  0.00108547  0.00158062  0.00084628  0.00128042  0.00079848\n",
      "   0.00123767  0.00102759  0.00142596  0.00101647  0.00090521  0.00174489\n",
      "   0.00109962  0.00148904  0.00157936  0.00117396  0.00096152  0.00087056\n",
      "   0.00111546  0.00130588  0.00199301  0.00083462  0.001119    0.00103787\n",
      "   0.00084234  0.00083582  0.0014036   0.0007611   0.0009789   0.00084701\n",
      "   0.00091636  0.00115742  0.00073584  0.00089941  0.00084901  0.00083834\n",
      "   0.00083784  0.0006428   0.00085025  0.00089436  0.00093183  0.00104738\n",
      "   0.0009497   0.00093312  0.00082766  0.00080359  0.00075328  0.00103079\n",
      "   0.00108856  0.00094698  0.00098878  0.00083473  0.00090564  0.00103229\n",
      "   0.00094635  0.0009086   0.00072438  0.00076669  0.00082959  0.00119874\n",
      "   0.00129292  0.00094804  0.0008494   0.00082936  0.00072859  0.00108263\n",
      "   0.00088312  0.00082684  0.00104035  0.00086832  0.00070049  0.00065899\n",
      "   0.00071427  0.00089475  0.000808    0.00060032  0.00105572  0.00071144\n",
      "   0.00079949  0.00066051  0.00080135  0.00069611  0.00078612  0.00054475\n",
      "   0.00078482  0.00064746  0.00092332  0.00086625  0.00074823  0.00078023\n",
      "   0.0007181   0.00081909  0.00063558  0.00077691  0.00077799  0.0006764\n",
      "   0.00069073  0.00078326  0.00088039  0.00097672  0.00080161  0.00078975\n",
      "   0.00084875  0.00078949  0.00071501  0.00076559  0.00079173  0.00085167\n",
      "   0.00061519  0.0006827   0.00075396  0.00087751  0.00091559  0.00076906\n",
      "   0.00113254  0.00070276  0.000632    0.00074299  0.00066077  0.00090877\n",
      "   0.00078915  0.00066437  0.00079837  0.00093717  0.00065167  0.00075826\n",
      "   0.00071174  0.00068804  0.00093626  0.00066255  0.00075906  0.0006997\n",
      "   0.00104398  0.0008972   0.00081632  0.00071278  0.00063409  0.00065165\n",
      "   0.00073283  0.00065423]]\n",
      "Input Text:\n",
      "Hans Rosling unveils data visuals that untangle the complex risk factors of one of the world's deadliest (and most misunderstood) diseases: HIV. By following the data, he suggests a surprising key to ending the epidemic. \n",
      "\n",
      "\n",
      "The model produced the title: visuals rosling data hans unveils \n"
     ]
    }
   ],
   "source": [
    "text = \"Hans Rosling unveils data visuals that untangle the complex risk factors of one of the world's deadliest (and most misunderstood) diseases: HIV. By following the data, he suggests a surprising key to ending the epidemic.\"\n",
    "generate_text_title(text,model,max_sequence_len,word_number_index,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
